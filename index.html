<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta name="description" content="ShareGPT4Video: Improving Video Understanding and Generation with Better Captions">
    <meta name="keywords" content="ShareGPT4Video">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>ShareGPT4Video</title>
  
    <link rel="icon" href="images/logo.jpg">
  
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="stylesheet" href="./static/css/leaderboard.css">
  
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script type="text/javascript" src="./static/js/sort-table.js" defer></script>
    <script src="./static/js/fontawesome.all.min.js" defer></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/explorer-index.js"></script>
    <script src="./static/js/question_card.js"></script>
    <script src="./static/js/leaderboard_testmini.js"></script>  
  </head>

  <body>

    <nav class="navbar" role="navigation" aria-label="main navigation">
      <div class="navbar-brand">
        <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
        </a>
      </div>
      <div class="navbar-menu">
        <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
          <div class="navbar-item has-dropdown is-hoverable">
            <a class="navbar-link">
              More Research
            </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://mmstar-benchmark.github.io/">
              <b><img src="images/mmstar.png" style="width:2.0em;vertical-align: middle" alt="Logo"/>MMStar</b>
            </a>
            <a class="navbar-item" href="https://sharegpt4v.github.io/">
              <b><img src="images/sharegpt4v.png" style="width:2.0em;vertical-align: middle" alt="Logo"/>ShareGPT4V</b>
            </a>
          </div>
        </div>
      </div>
    </nav>
    
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-2 publication-title"><img id="logo" width="8%" src="images/logo.jpg"> ShareGPT4Video:</h1>
              <h1 class="title is-2 publication-title">Improving Video Understanding and Generation with Better Captions</h1>
              <font size="5"><span style="color: red; font-weight: bold;">NeurIPS 2024 D&B Track</span></font>
              <div class="is-size-5 publication-authors">
                <br>
                <span class="author-block">
                  <a href="https://lin-chen.site/" style="font-weight:normal;">Lin Chen<b><sup>* &sect;1,4</sup></b></a>,
                </span>
                <span class="author-block">
                  <a href="https://github.com/Wiselnn570/" style="font-weight:normal;">Xilin Wei<b><sup>* &sect;4</sup></b></a>,
                </span>
                <span class="author-block">
                  <a href="https://li-jinsong.github.io/" style="font-weight:normal;">Jinsong Li<b><sup>* &sect;2,4</sup></b></a>,
                </span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=FscToE0AAAAJ&hl=en" style="font-weight:normal;">Xiaoyi Dong<sup>2,4</sup></a>,
                </span>
                <span class="author-block">
                  <a href="https://panzhang0212.github.io/" style="font-weight:normal;">Pan Zhang<sup>4</sup></a>,
                </span>
                <span class="author-block">
                  <a href="https://yuhangzang.github.io/" style="font-weight:normal;">Yuhang Zang<sup>4</sup></a>,
                </span>
                <span class="author-block">
                  <a href="https://lovesnowbest.site/" style="font-weight:normal;">Zehui Chen<sup>1,4</sup></a>,
                </span>
                <span class="author-block">
                  <a href="https://kennymckormick.github.io/" style="font-weight:normal;">Haodong Duan<sup>4</sup></a>,
                </span>
                <span class="author-block">
                  <a href="https://scholar.google.com.hk/citations?user=GCOVDKoAAAAJ&hl=en" style="font-weight:normal;">Bin Lin<sup>3</sup></a>,
                </span>
                <span class="author-block">
                  <a href="" style="font-weight:normal;">Zhenyu Tang<sup>3</sup></a>,
                </span>
                <span class="author-block">
                  <a href="https://yuanli2333.github.io/" style="font-weight:normal;">Li Yuan<sup>3</sup></a>,
                </span>
                <span class="author-block">
                  <a href="https://mmlab.siat.ac.cn/yuqiao/" style="font-weight:normal;">Yu Qiao<sup>4</sup></a>,
                </span>
                <span class="author-block">
                  <a href="http://dahua.site/" style="font-weight:normal;">Dahua Lin<sup>2,4</sup></a>,
                </span>
                <span class="author-block">
                  <a href="https://scholar.google.co.uk/citations?user=r6CvuOUAAAAJ&hl=en" style="font-weight:normal;">Feng Zhao<b><sup>&dagger;1</sup></b></a>,
                </span>
                <span class="author-block">
                  <a href="https://myownskyw7.github.io/" style="font-weight:normal;">Jiaqi Wang<b><sup>&dagger;4</sup></b></a>
                </span>
              </div>

              <div class="is-size-5 publication-authors">
                <span class="author-block"><b style="color:#008AD7; font-weight:normal">&#x25B6 </b> <sup>1</sup> University of Science and Technology of China</b></span>
                <span class="author-block"><b style="color:#fa7f6f; font-weight:normal">&#x25B6 </b> <sup>2 </sup> The Chinese University of Hong Kong</span>
                <span class="author-block"><b style="color:#f68946; font-weight:normal">&#x25B6 </b> <sup>3 </sup> Peking University</span>
                <span class="author-block"><b style="color:#008AD7; font-weight:normal">&#x25B6 </b> <sup>4 </sup> Shanghai AI Laboratory</span>
              </div>
              
              <div class="is-size-6 publication-authors">
                <br>
                <span class="author-block"><b>*</b> Equal contribution.</span>
                <span class="author-block"><b>&dagger;</b> Corresponding authors.</span>
              </div>
              
              <div class="is-size-6 publication-authors">
                <span class="author-block"><b><sup>&sect;</sup></b> Work done during an internship in Shanghai AI Laboratory.</span>
              </div>
              <div class="column has-text-centered">
                <div class="publication-links">
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2406.04325v1" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://github.com/ShareGPT4Omni/ShareGPT4Video" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://huggingface.co/datasets/ShareGPT4Video/ShareGPT4Video" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <p style="font-size:18px">ðŸ¤—</p>
                      </span>
                      <span>Dataset</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://huggingface.co/Lin-Chen/ShareCaptioner-Video" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <p style="font-size:18px">ðŸ¤—</p>
                      </span>
                      <span>ShareCaptioner-Video</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://huggingface.co/Lin-Chen/sharegpt4video-8b" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <p style="font-size:18px">ðŸ¤—</p>
                      </span>
                      <span>ShareGPT4Video-8B</span>
                    </a>
                  </span>
                  <br>
                  <span class="link-block">
                    <a href="https://huggingface.co/spaces/Lin-Chen/ShareCaptioner-Video" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <p style="font-size:18px">ðŸŽ¬</p>
                      </span>
                      <span>ShareCaptioner-Video Demo</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://huggingface.co/spaces/Lin-Chen/ShareGPT4Video-8B" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <p style="font-size:18px">ðŸŽ¬</p>
                      </span>
                      <span>ShareGPT4Video-8B Demo</span>
                    </a>
                  </span>
                </div>
                <font size="3">
                  <br>ðŸš€ <b>A large-scale highly descriptive</b> video-text dataset, with <b>40K</b> captions annotated by GPT4V and <b>4.8M</b> captions annotated by our ShareCaptioner-Video. The total videos last with <b>300</b> hours and <b>3000</b> hours separately!
                  <br>ðŸš€ <b>A general video captioner for various video durations, resolutions, aspect ratios</b>, approaching GPT4-Vision's caption capability, featuring two inference mode targeted for quality and efficiency, separately.
                  <br>ðŸš€ A superior large multi-modal model <b>ShareGPT4Video-8B</b>, lasting <b>5 hours</b> on 8xA100 GPUs of training only.
                  <br>ðŸš€ Improving <b>Text-to-Video performance</b> with high-quality video captions generate by our ShareCaptioner-Video
                </font>
                <br>
                <font size="6">
                  <br>ðŸ”¥<b>What's New</b>
                </font>
                <font size="4">
                  <table width="90%" align="center" border="0" cellspacing="0" cellpadding="0"><tbody>
                    <tr>
                      <td>
                        <div  align="left" style="height: 150px; overflow: auto;">
                          <ul>
                            <li> <span style="color: red; font-weight: bold;">[2024.09.26] ShareGPT4Video is accepted by NeurIPS 2024 D&B Track! ðŸŽ‰ðŸŽ‰ðŸŽ‰</span>
                            <li> <b style="color:#E68E34">[2024.07.01]</b> The code about <b>batch-inference</b> of <b>ShareCaptioner-Video</b> is available now!
                            <li> <b style="color:#E68E34">[2024.06.12]</b> The <b>Web Demo</b> and <b>Local Demo</b> of <b>ShareCaptioner-Video</b> are available now!
                            <li> <b style="color:#E68E34">[2024.06.11]</b> The <b>Web Demo</b> and <b>Local Demo</b> of <b>ShareGPT4Video-8B</b> are available now!
                            <li> <b style="color:#E68E34">[2024.06.07]</b> Our paper has been featured as <b>ðŸ¤—HuggingFace Daily Papers</b> and <b>ranked <b style="color:#ff0000">1st</b></b>.
                            <li> <b style="color:#E68E34">[2024.06.07]</b> The <b>Paper</b> is released!
                            <li> <b style="color:#E68E34">[2024.06.06]</b> The <b>ShareCaptioner-Video</b> model is released!
                            <li> <b style="color:#E68E34">[2024.05.27]</b> The <b>ShareGPT4Video-8B</b> model is released!
                            <li> <b style="color:#E68E34">[2024.05.08]</b> <b>Project Page</b> and <b>ShareGPT4Video Dataset</b> are released!</b>
                          </ul>
                        </div>
                      </td>
                    </tr>
                  </tbody></table>
                </font>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="hero is-light is-small" id="Video">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <br>
          <h2 class="title is-3">Demo Video</h2>
          <video width="45%" controls>
            <source src="videos/demo.mp4" type="video/mp4">
          </video>
          <br>
          <br>
        </div>
      </div>
    </section>

    <section class="section" id="Abstract">
      <div class="container" style="margin-bottom: 2vh;">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                We present the ShareGPT4Video series, aiming to facilitate the video understanding of large video-language models (LVLMs) and the video generation of text-to-video models (T2VMs) via dense and precise captions.
                The series comprises: <b>1) ShareGPT4Video</b>, 40K GPT4V annotated dense captions of videos with various lengths and sources, developed through carefully designed data filtering and annotating strategy.
                <b>2) ShareCaptioner-Video</b>, an efficient and capable captioning model for arbitrary videos, with 4.8M high-quality aesthetic videos annotated by it.
                <b>3) ShareGPT4Video-8B</b>, a simple yet superb LVLM that reached SOTA performance on three advancing video benchmarks.
                To achieve this, taking aside the non-scalable costly human annotators, we find using GPT4V to caption video with a naive multi-frame or frame-concatenation input strategy leads to less detailed and sometimes temporal-confused results. We argue the challenge of designing a high-quality video captioning strategy lies in three aspects: 
                <b>1) Inter-frame precise temporal change understanding.
                2) Intra-frame detailed content description.
                3) Frame-number scalability for arbitrary-length videos. </b>
                To this end, we meticulously designed a differential video captioning strategy, which is stable, scalable, and efficient for generating captions for videos with arbitrary resolution, aspect ratios, and length.
                Based on it, we construct ShareGPT4Video, which contains 40K high-quality videos spanning a wide range of categories, and the resulting captions encompass rich world knowledge, object attributes, camera movements, and crucially, detailed and precise temporal descriptions of events. 
                Based on ShareGPT4Video, we further develop ShareCaptioner-Video, a superior captioner capable of efficiently generating high-quality captions for arbitrary videos. We annotated 4.8M aesthetically appealing videos by it and verified their effectiveness on a 10-second text2video generation task.
                For video understanding, we verified the effectiveness of ShareGPT4Video on several current LVLM architectures and presented our superb new LVLM ShareGPT4Video-8B. All the models, strategies, and annotations(We do not hold the copyright for any video and will provide the link-annotation pair for research-only usage.) will be open-sourced and we hope this project can serve as a pivotal resource for advancing both the LVLMs and T2VMs community.
              </p>
            </div>
          </div>
        </div>
      </div>

    </section>
    
    <section class="hero is-light is-small" id="Dataset Title">
      <div class="hero-body has-text-centered">
        <h1 class="title is-2">
          <img src="images/logo.jpg" style="width:4%;vertical-align: middle" alt="Logo"/>
          <span style="vertical-align: middle">ShareGPT4Video Dataset</span>
        </h1>
      </div>
    </section>
    <section class="section" id="Dataset">
      <div class="container">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <div class="content has-text-justified">
              <b><font size=5>Details and attributes of the ShareGPT4Video</font></b>
              <br>
              <br>
              <centering>
                <div style="text-align: center;">
                  <img id="teaser" width="100%" src="images/teaser.png">
                  <p style="font-family:Times New Roman">
                    <font size=4>
                      (a) The proposed ShareGPT4Video dataset contains a large volume of high-quality video-caption pairs collected from diverse sources, with 40K captions from GPT4V and 4.8M captions from our ShareCaptioner-Video.
                      (b) We illustrate in detail the process of harnessing the multi-modal image model GPT4V to generate high-quality captions for videos.
                      (c) Our unique captioning strategy enables the re-caption of sub-clips by reusing their differential captions.
                    </font>
                  </p>
                </div>
              </centering>
              <br>

              <b><font size=5>Dataset generated by GPT-4V</font></b>
              <br>
              <br>
              <centering>
                <style>
                  table.GeneratedTable {
                    width: 100%;
                    background-color: #ffffff;
                    border-collapse: collapse;
                    border-width: 2px;
                    border-color: #c1c4c5;
                    border-style: solid;
                    color: #000000;
                  }
                  
                  table.GeneratedTable td, table.GeneratedTable th {
                    border-width: 2px;
                    border-color: #9b9d9e;
                    border-style: solid;
                    padding: 3px;
                  }
                  
                  table.GeneratedTable thead {
                    background-color: #6691ee;
                  }
                </style>
                <div class="column is-six-fifths" width="80%">
                  <table class="GeneratedTable">
                    <thead>
                      <tr>
                        <th>Data Source</th>
                        <th>Samples</th>
                        <th>Total Time(hours)</th>
                        <th>Avg. Time(sec)</th>
                        <th>Avg. Length(#word)</th>
                      </tr>
                    </thead>
                    <tbody>
                      <tr>
                        <td>Panda-70M</td>
                        <td>27092</td>
                        <td>204.4</td>
                        <td>27.2</td>
                        <td>291.2</td>
                      </tr>
                      <tr>
                        <td>Pexels</td>
                        <td>8487</td>
                        <td>52.2</td>
                        <td>22.1</td>
                        <td>254.9</td>
                      </tr>
                      <tr>
                        <td>Pixabay</td>
                        <td>2725</td>
                        <td>20.3</td>
                        <td>26.9</td>
                        <td>209.3</td>
                      </tr>
                      <tr>
                        <td>BDD100K</td>
                        <td>608</td>
                        <td>6.6</td>
                        <td>39.0</td>
                        <td>371.3</td>
                      </tr>
                      <tr>
                        <td>Mixkit</td>
                        <td>745</td>
                        <td>3.6</td>
                        <td>17.5</td>
                        <td>213.9</td>
                      </tr>
                      <tr>
                        <td>Ego4D</td>
                        <td>521</td>
                        <td>3.9</td>
                        <td>27.1</td>
                        <td>298.9</td>
                      </tr>
                      <tr bgcolor="#a4cff4">
                        <td>Total</td>
                        <td>40178</td>
                        <td>291</td>
                        <td>26.6</td>
                        <td>273.3</td>
                      </tr>
                    </tbody>
                  </table>
                  <div style="text-align: center;">
                      <img id="pie" width="100%" src="images/caption_pie.png">
                      <p style="font-family:Times New Roman">
                        <font size=4>
                          <b>Comprehensive video-caption dataset</b>: 
                          (a) The dataset covers a broad spectrum of content, including
                          wildlife, cooking, sports, scenery, ego-centric human activities, auto-driving scenarios, etc. 
                          (b) The dataset includes videos ranging from 2 seconds to 2 minutes in length. 
                          (c) The captions primarily range from 200 to 400 words, providing rich temporal information that serves video understanding and generation tasks well.
                        </font>
                    </div>
                  </div>
              </centering>
              <br>

            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="hero is-light is-small" id="Captioner Title">
      <div class="hero-body has-text-centered">
        <h1 class="title is-2">
          <img src="images/logo.jpg" style="width:4%;vertical-align: middle" alt="Logo"/>
          <span style="vertical-align: middle">ShareCaptioner-Video</span>
        </h1>
      </div>
    </section>
    <section class="section" id="Captioner">
      <div class="container">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <div class="content has-text-justified">

              <b><font size=5>ShareCaptioner-Video with 4 capabilities</font></b>
              <br>
              <br>
              <centering>
                <div style="text-align: center;">
                  <img id="captioner" width="100%" src="images/sharecaptioner.png">
                  <p style="font-family:Times New Roman">
                    <font size=4>
                      The ShareCaptioner-Video is a Four-in-One exceptional video captioning model with the following capabilities: Fast captioning, Sliding Captioning, Clip Summarizing, and Prompt Re-Captioning
                    </font>
                  </p>
                </div>
              </centering>
              <br>

              <b><font size=5>Dataset generated by ShareCaptioner-Video</font></b>
              <br>
              <br>
              <centering>
                <style>
                  table.GeneratedTable {
                    width: 100%;
                    background-color: #ffffff;
                    border-collapse: collapse;
                    border-width: 2px;
                    border-color: #c1c4c5;
                    border-style: solid;
                    color: #000000;
                  }
                  
                  table.GeneratedTable td, table.GeneratedTable th {
                    border-width: 2px;
                    border-color: #9b9d9e;
                    border-style: solid;
                    padding: 3px;
                  }
                  
                  table.GeneratedTable thead {
                    background-color: #6691ee;
                  }
                </style>
                <div class="column is-six-fifths" width="80%">
                  <table class="GeneratedTable">
                    <thead>
                      <tr>
                        <th>Data Source</th>
                        <th>Samples</th>
                        <th>Total Time(hours)</th>
                        <th>Avg. Length(#word)</th>
                      </tr>
                    </thead>
                    <tbody>
                      <tr>
                        <td>Mixkit</td>
                        <td>56k</td>
                        <td>42.0</td>
                        <td>104.8</td>
                      </tr>
                      <tr>
                        <td>Pixabay</td>
                        <td>652k</td>
                        <td>353.3</td>
                        <td>102.5</td>
                      </tr>
                      <tr>
                        <td>Pexels</td>
                        <td>4104k</td>
                        <td>2561.9</td>
                        <td>100.5</td>
                      </tr>
                      <tr bgcolor="#a4cff4">
                        <td>Total</td>
                        <td>4812k</td>
                        <td>2957.2</td>
                        <td>102.6</td>
                      </tr>
                    </tbody>
                  </table>
                  <div align="center">
                    <p style="font-family:Times New Roman">
                      <font size=4>
                        Statics of 4.8M high-quality video-caption pairs generated by our ShareCaptioner-Video.
                      </font>
                    </p>  
                  </div>
                </div>
              </centering>
              <br>

              <b><font size=5>A Comparison of caption quality from various sources</font></b>
              <br>
              <br>
              <div style="text-align: center;">        
                <img id="teaser" width="60%" src="images/sharecaptioner_compare.png">
                <p style="font-family:Times New Roman">
                  <font size=4>
                    Mistakes within the captions are highlighted in red, whereas detailed and accurate parts are emphasized in blue
                  </font>
                </p>
              </div>
                    
            </div>
          </div>
        </div>
      </div>
    </section>


          <section class="hero is-light is-small" id="Dataset Title">
            <div class="hero-body has-text-centered">
            <h1 class="title is-2">
              <span style="vertical-align: middle">Text-to-Video Cases</span>
            </h1>
            </div>
          </section>
                

      <section class="section" id="Dataset">
        <div class="container">
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <div class="content has-text-justified">
                <p>We have generated a large volume of video-caption pairs with our ShareCaptioner-Video and trained a text-to-video model with the <a href="https://github.com/PKU-YuanGroup/Open-Sora-Plan/">Open-Sora-Plan repository</a>. Here are some interesting cases:</p>
                <div id="results-carousel" class="carousel results-carousel">
                  <div class="box m-5">
                    <div class="content has-text-centered">
                      <video width="520" height="360" controls>
                        <source src="videos/generation_case_1.mp4" type="video/mp4">
                      </video>
                      <div class="content has-text-justified">The video captures the spectacle of a continuous fireworks show against the backdrop of a starry night sky. It commences with a burst of vibrant reds, greens, purples, and yellows that paint the heavens and cast shimmering reflections upon the water below. As the display progresses, the fireworks evolve, transitioning from the initial array to a focus on radiant oranges, yellows, and fiery reds. These explosions form captivating clusters at the heart of the sky, ascending in breathtaking formations accompanied by trailing plumes of smoke, adding a dramatic flourish to the visual narrative. Throughout the duration, the fireworks maintain their dynamic allure, their patterns and positions evolving to underscore the ongoing spectacle. Meanwhile, the mirrored reflections on the water's surface faithfully echo the colors and shapes above, further enhancing the mesmerizing and ever-changing nature of the display.</div>
                    </div>
                  </div>
                  <div class="box m-5">
                    <div class="content has-text-centered">
                      <video width="520" height="360" controls>
                        <source src="videos/generation_case_2.mp4" type="video/mp4">
                      </video>
                      <div class="content has-text-justified">A drone camera circles around a beautiful historic church built on a rocky outcropping along the Amalfi Coast, the view showcases historic and magnificent architectural details and tiered pathways and patios, waves are seen crashing against the rocks below as the view overlooks the horizon of the coastal waters and hilly landscapes of the Amalfi Coast ltaly, several distant people are seen walking and enjoying vistas on patios of the dramatic ocean views, the warm glow of the afternoon sun creates a magical and romantic feeling to the scene, the view is stunning captured with beautiful photography.</div>
                    </div>
                  </div>
                  <div class="box m-5">
                    <div class="content has-text-centered">
                      <video width="520" height="360" controls>
                        <source src="videos/generation_case_3.mp4" type="video/mp4">
                      </video>
                      <div class="content has-text-justified">The video presents an abstract composition centered around a hexagonal shape adorned with a starburst pattern of lines, undergoing a series of transformations against a dark backdrop. Initially dominated by shades of blue, particularly within the central hexagon displaying a spiral or tunnel-like motif, the imagery gradually transitions through a spectrum of warm and cool tones. Pink, purple, red, and orange hues are introduced, creating lively contrasts with the blues as the composition evolves. These color shifts generate a dynamic interplay between warm and cool tones, with blues, reds, oranges, and later purples taking turns in prominence, each contributing to distinct visual effects. From cool blues, the palette progresses to warmer tones before returning to a balanced mix of reds and blues, ultimately settling back into cooler blues and purples. Throughout these changes, the central hexagon maintains its spiral or tunnel-like quality, drawing focus towards the center of the frame. The design elements exhibit subtle movements akin to gentle pulsations or breathing, infusing the composition with a sense of dynamism and vitality. Despite these shifts, the geometric and crystalline structure remains intact, occasionally sharpening to enhance clarity. The video concludes with a harmonious blend of blues and purples, offering a serene contrast to the earlier vibrant color combinations while retaining depth through the interplay of light and shadow.</div>
                    </div>
                  </div>
                </div>
            </div> 
          </div>
        </div>
      </div>
    </section>

    <section class="hero is-light is-small" id="Caption Case Title">
      <div class="hero-body has-text-centered">
      <h1 class="title is-2">
        <span style="vertical-align: middle">Video Caption Cases</span>
      </h1>
      </div>
    </section>
    <section class="section" id="Caption Case">
      <div class="container">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <div class="content has-text-justified">
              <div id="results-carousel" class="carousel results-carousel">
                <!-- <div class="box m-5">
                  <div class="content has-text-centered">
                    <video width="520" height="360" controls>
                      <source src="videos/117e30c815ff0f43c2c2069550cb4797e539c91c8af9ec38c90243cad13397bc.mp4" type="video/mp4">
                    </video>
                    <div class="content has-text-justified">The video depicts the process of a character emerging from the edge of a wall, interacting with the audience, and then departing, against a white background. Initially, this character's metallic helmet with golden wings appears on the edge. Later, the character in blue and white emerges, pointing off-screen with his right arm. Subsequently, the character adjusts his posture, appearing to interact with the audience outside the video. Finally, the character moves towards the video's edge.</div>
                  </div>
                </div>
                -->
                <div class="box m-5">
                  <div class="content has-text-centered">
                    <video width="520" height="360" controls>
                      <source src="videos/caption_case_1.mp4" type="video/mp4">
                    </video>
                    <div class="content has-text-justified">The video segment documented a significant event in Kochi, Kerala, where 2 buildings razed in Kochi. The broadcast began with a split-screen presentation: on one side, thick clouds of dust were seen billowing into the sky, marking the onset of the demolition process, while on the other side, reporter Gopikrishnan provided live coverage, indicated by "BREAKING NEWS" captions and a consistent timestamp of "11:10 AM." The news ticker at the bottom of the screen simultaneously ran other global events, maintaining a flow of information. As the video progresses, the split-screen footage of the razed house turns into a close-up. A notable change in the headline to "KOCHI FLATS RAZED" signaled the demolition's culmination. A brief interlude offered a visual contradiction by showcasing the flats presumably before their demolition, providing a stark before and after comparison. As the video progressed, the left building's collapse initiated a dramatic alteration in the skyline, marked by significant dust plumes. Subsequently, another building was shown partially collapsing amid debris, fully obscured by dust in seconds, with surrounding greenery remaining untouched. This transitioned into a graphic interlude featuring the "India Today" logo, briefly pausing the live footage. Resuming to the aftermath, split imagery displayed the rubble and ongoing smoke. Then, the imagery continued to juxtapose the scenes of destruction against intact high-rise buildings nearby. The narrative was augmented by the revelation that the Supreme Court directed the demolition within a broader national news context. Throughout, the report maintained a real-time approach, threading continuity and urgency across the unfolding event's documentation.</div>
                  </div>
                </div>
                <div class="box m-5">
                  <div class="content has-text-centered">
                    <video width="520" height="360" controls>
                      <source src="videos/caption_case_2.mp4" type="video/mp4">
                    </video>
                    <div class="content has-text-justified">The video begins with an individual seated on a gray couch in a cozy domestic setting, about to unbox a product from a red CCM-branded box placed on a white table in front of them. Initially, the person is seen drinking from a blue can, indicating a casual atmosphere. Soon after, the individual shifts attention from the can to the red box, signifying the start of the unboxing process. The red box, initially closed, gradually becomes the focal point as the person prepares to open it, conveying a build-up of anticipation. As the video progresses, the box is flipped over and then opened, revealing its content still hidden under white tissue paper adorned with prints, adding to the suspense. The individualâ€™s engagement with the box evolves, from initially preparing to open it, to actively delving into its contents. A momentary pause in activity is captured before the anticipation culminates with the individual lifting an object from the box. This object, identifiable by a yellow label, is then examined closely by the person, indicating a thorough inspection or perusal of the product or its packaging. Throughout the video, the surrounding environment remains consistent and undisturbed, with household items like a potted plant and a wall clock maintaining the setting's homely ambiance. The cameraâ€™s perspective remains fixed, focusing on the unfolding unboxing event without any movement, thus allowing the viewer to observe the narrative closely. Another partially open brown box is visible beside the main red box, though its role or contents are not elaborated upon. The video encapsulates the anticipation, action, and reveal inherent to unboxing experiences in a home setting.</div>
                  </div>
                </div>
              </div>
          </div> 
        </div>
      </div>
    </div>
  </section>
    
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title is-3 has-text-centered">ðŸ“ƒ BibTeX</h2>
        <pre><code>
          @article{chen2024sharegpt4video,
            title={ShareGPT4Video: Improving Video Understanding and Generation with Better Captions},
            author={Chen, Lin and Wei, Xilin and Li, Jinsong and Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Chen, Zehui and Duan, Haodong and Lin, Bin and Tang, Zhenyu and Yuan, Li and Qiao, Yu and Lin, Dahua and Zhao, Feng and Wang, Jiaqi},
            journal={arXiv preprint arXiv:2406.04325},
            year={2024}
          }
        </code></pre>
        <br>
      </div>
    </section>

    <footer class="footer">
      <div class="content has-text-centered">
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website template is adapted from <a href="https://sharegpt4v.github.io/">ShareGPT4V</a>, <a href="https://mmstar-benchmark.github.io/">MMStar</a> and <a href="https://nerfies.github.io/">Nerfies</a>, licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>
            </p>
            <centering>
              <div style="width: 30%; text-align: center;">
                <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=DDmpsVeUO0mQXQFfT33dAl8ROkUr79QB8ZwzE8x7EH8"></script>
              </div>
            </centering>
          </div>
        </div>
      </div>
    </footer>
    </body>
    </html>
